{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import tiktoken\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.core import FrozenDict\n",
    "import jax\n",
    "import optax\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "  vocab_size: int = 50304\n",
    "  n_head: int = 12\n",
    "  n_embd: int = 768\n",
    "  block_size: int = 1024\n",
    "  n_layer: int = 12\n",
    "  dropout_rate: float = 0.1\n",
    "  gradient_accumulation_steps: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "  config: ModelConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "\n",
    "    assert len(x.shape) == 3\n",
    "\n",
    "    b, l, d = x.shape\n",
    "\n",
    "    q     = nn.Dense(self.config.n_embd)(x)\n",
    "    k     = nn.Dense(self.config.n_embd)(x)\n",
    "    v     = nn.Dense(self.config.n_embd)(x)\n",
    "    # q*k / sqrt(dim) -> softmax -> @v\n",
    "    q     = jnp.reshape(q, (b, l, d//self.config.n_head , self.config.n_head))\n",
    "    k     = jnp.reshape(k, (b, l, d//self.config.n_head , self.config.n_head))\n",
    "    v     = jnp.reshape(v, (b, l, d//self.config.n_head , self.config.n_head))\n",
    "    norm  = jnp.sqrt(list(jnp.shape(k))[-1])\n",
    "    attn  = jnp.matmul(q,jnp.transpose(k, (0,1,3,2))) / norm\n",
    "    mask  = jnp.tril(attn)\n",
    "    attn  = jnp.where(mask[:,:,:l,:l], attn, float(\"-inf\"))\n",
    "    probs = jax.nn.softmax(attn, axis=-1)\n",
    "    y     = jnp.matmul(probs, v)\n",
    "    y     = jnp.reshape(y, (b,l,d))\n",
    "    y     = nn.Dense(self.config.n_embd)(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "  config: ModelConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    x = nn.Dense(self.config.n_embd*4)(x)\n",
    "    x = nn.gelu(x, approximate=True)\n",
    "    x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = nn.Dense(self.config.n_embd)(x)\n",
    "    x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=deterministic)\n",
    "    return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "  config: ModelConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.LayerNorm()(x)\n",
    "    x = x + CausalSelfAttention(self.config)(x)\n",
    "    x = nn.LayerNorm()(x)\n",
    "    x = x + MLP(self.config)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "  config: ModelConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=False):\n",
    "    \n",
    "    B, T = x.shape\n",
    "    assert T <= self.config.block_size\n",
    "\n",
    "    pos     = jnp.arange(0, T)[None]\n",
    "    pos_emb = nn.Embed(self.config.block_size, self.config.n_embd)(pos)\n",
    "    wte     = nn.Embed(self.config.vocab_size, self.config.n_embd)\n",
    "    tok_emb = wte(x)\n",
    "    x       = tok_emb + pos_emb\n",
    "\n",
    "    for _ in range(self.config.n_layer):\n",
    "      x = Block(self.config)(x)\n",
    "    x = nn.LayerNorm()(x)\n",
    "    # logits = nn.Dense(config.n_embd, config.vocab_size)(x)\n",
    "    logits = wte.attend(x) # parameter sharing\n",
    "    return logits\n",
    "  \n",
    "  def init(self, rng):\n",
    "    tokens = jnp.zeros((1, self.config.block_size), dtype=jnp.uint16)\n",
    "    params = jax.jit(super().init, static_argnums=(2,))(rng, tokens, True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(params):\n",
    "    p = jax.tree_util.tree_map(lambda a : a.size if isinstance(a, jnp.ndarray) else 0, params)\n",
    "    return jax.tree_util.tree_reduce(lambda a,b : a+b, p)\n",
    "config = ModelConfig()\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = GPT(config)\n",
    "params = model.init(key)\n",
    "# count_params(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['params'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "  def __init__(self, B, T):\n",
    "    self.current_position = 0\n",
    "    self.B = B\n",
    "    self.T = T\n",
    "\n",
    "    with open(\"input.txt\",\"r\") as f:\n",
    "      text = f.read()\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    self.tokens = jnp.array(enc.encode(text))\n",
    "    print(f\"loaded {len(self.tokens)} tokens in the datasets\" )\n",
    "    print(f\" 1 epoch = {len(self.tokens)//(B*T)} batches\")\n",
    "\n",
    "  def next_batch(self):\n",
    "    B,T = self.B, self.T\n",
    "    buf = self.tokens[self.current_position:self.current_position+B*T+1]\n",
    "    x,y = jnp.reshape(buf[:-1],(B,T)), jnp.reshape(buf[1:],(B,T))\n",
    "    self.current_position += B*T\n",
    "    if self.current_position + B*T+1 > len(self.tokens):\n",
    "      self.current_position = 0\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(B=4, T=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_state(key, config) -> TrainState:\n",
    "  # model = GPT(config)\n",
    "  # gradient checkpointing\n",
    "  model = nn.remat(\n",
    "        GPT, policy=jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims\n",
    "        )(config)\n",
    "  params = model.init(key)\n",
    "  optimizer = optax.adamw(3e-4, b1=0.9, b2=0.98, eps=1e-9, weight_decay=1e-1)\n",
    "  learning_rate = optax.warmup_cosine_decay_schedule(\n",
    "      init_value=0.0,\n",
    "      peak_value=2.5e-4,\n",
    "      warmup_steps= 2000,\n",
    "      decay_steps= 150000,\n",
    "      end_value = 1e-5,\n",
    "      )\n",
    "  optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0), # gradient clipping\n",
    "            optax.adamw(learning_rate, b1=0.9, b2=0.95,  weight_decay=1e-2)\n",
    "      )\n",
    "  if config.gradient_accumulation_steps>1:\n",
    "    optimizer = optax.MultiSteps(\n",
    "          optimizer, every_k_schedule=config.gradient_accumulation_steps\n",
    "    )\n",
    "  train_state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer)\n",
    "  return train_state\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: TrainState, x: jnp.ndarray, y: jnp.ndarray) -> Tuple[jnp.ndarray, TrainState]:\n",
    "\n",
    "  def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
    "\n",
    "      logits = state.apply_fn(params, x, False)\n",
    "      loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
    "      return loss\n",
    "\n",
    "  loss, grads = jax.value_and_grad(loss_fn, has_aux=False)(state.params)\n",
    "  new_state = state.apply_gradients(grads=grads)\n",
    "  return loss, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 50\n",
    "x,y = data_loader.next_batch()\n",
    "train_state = init_train_state(key, config)\n",
    "for step in range(train_steps):\n",
    "    t0 = time.time()\n",
    "    for _ in range(config.gradient_accumulation_steps):\n",
    "        x,y = data_loader.next_batch()\n",
    "        loss, train_state = train_step(train_state, x, y)\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    token_processed = data_loader.B * data_loader.T * config.gradient_accumulation_steps\n",
    "    tokens_per_sec = token_processed / dt\n",
    "    print(f\"step {step}/{train_steps} | loss : {loss:4f} | dt: {dt*1000:.2f} ms | token/sec = {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
